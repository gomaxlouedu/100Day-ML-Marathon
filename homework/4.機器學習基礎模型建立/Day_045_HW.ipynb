{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging: Sampling N examples with replacement\n",
    "1. 迴歸：用Average, 分類：用voting\n",
    "2. 應用於模型複雜，容易有overfit的情形時，可減少variance\n",
    "\n",
    "Boosting: 錯誤率>50%, 經過boosting之後，錯誤率為0\n",
    "1. Improving Weak Classifiers\n",
    "2. The classifiers are learned sequentially\n",
    "3. 先找到f1(x), 然後再找一個盡量互補於f1(x)的f2(x), 再找互補於f2(x)的f3(x)，依此類推\n",
    "4. obtain different classifiers: Re-sampling, Re-weighting => change the cost/objective function\n",
    "> Adaboost：訓練資料的error雖然已至0 , 但增加weak classifier的數量，可使測試資料的error 下降\n",
    "\n",
    "Gradient Boosting:類似Gradient Decent, 但可調整object function\n",
    "\n",
    "Stacking: 各個模型的的output 成為final classifier的input, 初始模型及final classifier的training data 必須不同，final classifier 會根據新的training data 調整各個模型的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
